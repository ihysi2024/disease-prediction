{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Graph Convolution Networks to Identify Patient Disease Based on Symptomology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://arxiv.org/abs/2205.09148 -> download release_conditions.json\n",
    "\n",
    "Tutorials followed:\n",
    "\n",
    "https://medium.com/cj-express-tech-tildi/first-timers-guide-to-pytorch-geometric-part-1-the-basic-1b6006e1f4db \n",
    "\n",
    "https://towardsdatascience.com/a-beginners-guide-to-graph-neural-networks-using-pytorch-geometric-part-1-d98dc93e7742 \n",
    "\n",
    "https://towardsdatascience.com/a-beginners-guide-to-graph-neural-networks-using-pytorch-geometric-part-2-cd82c01330ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import os.path as osp\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch geometric imports\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import add_self_loops, degree, from_networkx, to_networkx\n",
    "from torch_geometric.data import InMemoryDataset, download_url, Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# graph-specific imports\n",
    "import networkx as nx\n",
    "from GraphEmbedding.ge import DeepWalk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GCNConv from torch.nn \n",
    "\n",
    "** note - removed dropout layer between nonlinear activation layer (F.relu) and second convolutional layer (self.conv2)\n",
    "The dropout layer is usually implemented in dense convolution networks when it is computationally expensive to fit all possible neural networks to a given dataset. This dataset is smaller and does not need dropout (performance drops as network thins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial dataset of disease and phenotypic data\n",
    "\n",
    "f = open('/gstore/scratch/u/hysii/nlp/original_dataset')\n",
    "json_data = json.load(f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only focusing on diseases and symptoms, some symptoms repeat across multiple diseases -> focus on unique symptomology that distinguishes each disease "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, extract all symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sympts = []\n",
    "\n",
    "for key, other_info in json_data.items():\n",
    "    for s in other_info['symptoms'].keys():\n",
    "        all_sympts.append(s)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, determine which symptoms never occur for more than one disease/condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sympts = []\n",
    "for i, sympt in enumerate(all_sympts):\n",
    "    counter = 0\n",
    "    \n",
    "    for idx, s in enumerate(all_sympts):\n",
    "        if sympt == s and i != idx:\n",
    "            counter += 1\n",
    "    \n",
    "    if counter == 0:\n",
    "        unique_sympts.append(sympt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dictionary that maps each disease/condition to their unique symptomology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "larmes\n",
      "vo_violent\n",
      "ww_bouger\n",
      "pyrosis\n",
      "ww_bouffe\n",
      "pale\n",
      "rectorragie\n",
      "ménorr\n",
      "ballon_abdo\n",
      "ww_valsalva\n",
      "obstipation\n",
      "dysarthrie\n",
      "diplopie\n",
      "fatigabilité_msk\n",
      "claud_mâchoire\n",
      "posttus_emesis\n",
      "insp_siffla\n",
      "contact_allergie\n",
      "bw_bending\n",
      "faiblesse faciale\n",
      "footnumb\n",
      "paralysie_visage\n",
      "toux_Aboy\n",
      "pls_irreg\n",
      "prurit_occ\n",
      "prurit_nasal\n",
      "rds_anorexie\n",
      "flushing\n",
      "dysp_effort\n",
      "laryngospasme\n",
      "spasmes_msk\n",
      "trismus\n",
      "spasme_trapeze\n",
      "protu_langue\n",
      "regard_dévié\n",
      "gain_poids\n",
      "ulcères_bouche\n",
      "angor_accelere\n",
      "rds_sg\n",
      "confusion\n",
      "etouff\n",
      "psy_depers\n",
      "impression_mort\n",
      "boire_ped\n",
      "apnee\n",
      "erytheme_occ\n",
      "convulsion\n",
      "pertes_vag\n",
      "selles_pale\n"
     ]
    }
   ],
   "source": [
    "path_to_symptoms = {}\n",
    "\n",
    "for key, other_info in json_data.items():\n",
    "    symp_lst = []\n",
    "    for s in unique_sympts:\n",
    "        if s in other_info['symptoms'].keys():\n",
    "            print(s)\n",
    "            symp_lst.append(s)\n",
    "    \n",
    "    if len(symp_lst) != 0:\n",
    "        path_to_symptoms[key] = symp_lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a new dataset to rely on with only the unique symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/gstore/scratch/u/hysii/nlp/unique_symptoms.json', 'w') as f:\n",
    "    json.dump(path_to_symptoms, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/gstore/scratch/u/hysii/nlp/unique_symptoms.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using networkx's builtin graph functionality, create a graph from our updated json file and store it in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph(data)\n",
    "\n",
    "with open('/gstore/scratch/u/hysii/nlp/graph.pckl', 'wb') as f:\n",
    "    graph = pickle.dump(G, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using DeepWalk, generate node embeddings that are more informed based on vertex representations in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning embedding vectors...\n",
      "Learning embedding vectors done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/gstore/data/dsi_ai_analytics/projects/cv/Segmentation/envs/ira_ssl/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3473: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# train the model and generate embeddings\n",
    "model = DeepWalk(G, walk_length=5, num_walks=15, workers=1)\n",
    "model.train(window_size=5,iter=3)\n",
    "\n",
    "embeddings = model.get_embeddings()\n",
    "embeddings = np.stack(embeddings.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify diseases based on symptoms, generate an array of labels where each disease corresponds to a label\n",
    "\n",
    "list of nodes = ['Flu', 'cough', 'sore throat', 'Pink eye', 'redness', 'itchiness']\n",
    "\n",
    "(Flu, cough, sore throat) -> label 0\n",
    "\n",
    "(pink eye, redness, itchiness) -> label 1\n",
    "\n",
    "list of labels = [0, 0, 0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for idx, (key, u_sympts) in enumerate(path_to_symptoms.items()):\n",
    "    labels.append(idx)\n",
    "    for s in u_sympts:\n",
    "        labels.append(idx)\n",
    "\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Functional Custom Dataset Using Previous Embeddings and Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(MyDataset, self).__init__(root, transform, None, None)\n",
    "\n",
    "        # turn networkx graph into a torch.data object \n",
    "        data = from_networkx(G)\n",
    "\n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # embedding \n",
    "        data.x = torch.from_numpy(embeddings).type(torch.float32)\n",
    "        \n",
    "        # labels\n",
    "        y = torch.from_numpy(labels).type(torch.long)\n",
    "        data.y = y.clone().detach()\n",
    "        \n",
    "        # number of classes/labels = number of diseases\n",
    "        data.num_classes = len(path_to_symptoms.keys())\n",
    "\n",
    "        # splitting the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(pd.Series(list(G.nodes)), \n",
    "                                                            pd.Series(labels),\n",
    "                                                            test_size=0.30, \n",
    "                                                            random_state=42)\n",
    "        \n",
    "        n_nodes = G.number_of_nodes()\n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[X_train.index] = True\n",
    "        test_mask[X_test.index] = True\n",
    "        data['train_mask'] = train_mask\n",
    "        data['test_mask'] = test_mask\n",
    "\n",
    "        self.data, self.slices = self.collate([data])\n",
    "        \n",
    "    def _download(self):\n",
    "        return\n",
    "\n",
    "    def _process(self):\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(Path('/gstore/scratch/u/hysii/nlp/graph.pckl'))\n",
    "data = dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set The Model To Train On To The Graph Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data =  data.to(device)\n",
    "model = Net().to(device) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement training and apply to test set to output accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Train Accuracy: 0.8918918918918919\n",
      "Test Accuracy: 0.23684210526315788\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "optimizer_name = \"Adam\"\n",
    "lr = 1e-1\n",
    "optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "epochs = 200\n",
    "\n",
    "def train():\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "  optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "  model.eval()\n",
    "  logits = model()\n",
    "  mask1 = data['train_mask']\n",
    "  pred1 = logits[mask1].max(1)[1]\n",
    "  acc1 = pred1.eq(data.y[mask1]).sum().item() / mask1.sum().item()\n",
    "  mask = data['test_mask']\n",
    "  pred = logits[mask].max(1)[1]\n",
    "  acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "  return acc1,acc\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "  train()\n",
    "\n",
    "train_acc,test_acc = test()\n",
    "\n",
    "print('#' * 70)\n",
    "print('Train Accuracy: %s' %train_acc )\n",
    "print('Test Accuracy: %s' % test_acc)\n",
    "print('#' * 70)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future direction:\n",
    "\n",
    "Generate a dataset that incorporates genetic variants that may contribute to each disease\n",
    "\n",
    "Implement GraphSAGE and GAT to compare against"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira_ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1c9c9244ec912227f843a1ca0e88552f3e3ecc30229065d9dfc3f64de041fb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
